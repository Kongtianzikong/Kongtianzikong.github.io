<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Hexo</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
<meta name="generator" content="Hexo 6.3.0"></head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/archives" class="header__link">Archive</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Hexo</a></h1>
		<h2 class="header__subtitle"></h2>
	</header>

	<main>
		



	<article>
	
		<h1><a href="/2023/06/23/Kafka/"></a></h1>
	
	<div class="article__infos">
		<span class="article__date">2023-06-23</span><br />
		
		
	</div>

	

	
		
	

	

</article>




	<article>
	
		<h1><a href="/2023/06/13/Spark StandAlone/">Spark StandAlone搭建</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2023-06-13</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark/" rel="tag">Spark</a> <a class="article__tag-none-link" href="/tags/%E6%90%AD%E5%BB%BA/" rel="tag">搭建</a>
			</span>
		
	</div>

	

	
		<h1 id="Spark-StandAlone搭建"><a href="#Spark-StandAlone搭建" class="headerlink" title="Spark StandAlone搭建"></a>Spark StandAlone搭建</h1><ol>
<li>在所有机器上安装python（Anaconda）<br>同时不要忘记 都创建pysapek虚拟环境 以及安装虚拟环境所需要的包pyspark jieba pyhive<br>2 .在所有机器配置环境变量<br>参考local模式下环境变量的配置内容<br>3.配置配置文件<br>进入到spark的配置文件目录中, cd $SPARK_HOME&#x2F;conf<br>配置workers文件</li>
</ol>
<p>改名, 去掉后面的.template后缀</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/1.png"></figure></p>
<p>编辑worker文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure>
<p>将里面的localhost删除, 追加</p>
<p>node1</p>
<p>node2</p>
<p>node3</p>
<p>到workers文件内</p>
<p><figure class="figure"><img src="/../image-sparkstandalone/2.png"></figure><br>配置spark-env.sh文件</p>
<ol>
<li><p>改名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/3.png"></figure></p>
</li>
<li><p>编辑spark-env.sh, 在底部追加如下内容</p>
</li>
</ol>
<p>设置JAVA安装目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/export/server/jdk</span><br></pre></td></tr></table></figure>
<p>HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>
<p>指定spark老大Master的IP和提交任务的通信端口<br>告知Spark的master运行在哪个机器上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_HOST=node1</span><br></pre></td></tr></table></figure>
<p>告知sparkmaster的通讯端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>
<p>告知spark master的 webui端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br></pre></td></tr></table></figure>
<p>worker cpu可用核数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_CORES=1</span><br></pre></td></tr></table></figure>
<p>worker可用内存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_MEMORY=1g</span><br></pre></td></tr></table></figure>
<p>worker的工作通讯地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_PORT=7078</span><br></pre></td></tr></table></figure>
<p>worker的 webui地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br></pre></td></tr></table></figure>
<h2 id="设置历史服务器"><a href="#设置历史服务器" class="headerlink" title="设置历史服务器"></a>设置历史服务器</h2><p>配置的意思是  将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/../image-sparkstandalone/4.png"></figure><br>在HDFS上创建程序运行历史记录存放的文件夹:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/5.png"></figure><br>配置spark-defaults.conf文件</p>
<ol>
<li>改名<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure>
<figure class="figure"><img src="/../image-sparkstandalone/6.png"></figure></li>
<li>修改内容, 追加如下内容<br>开启spark的日期记录功能<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled 	true</span><br></pre></td></tr></table></figure>
设置spark日志记录的路径<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.dir	 hdfs://node1:8020/sparklog/ </span><br></pre></td></tr></table></figure>
设置spark日志是否启动压缩<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.compress 	true</span><br></pre></td></tr></table></figure>
<figure class="figure"><img src="/../image-sparkstandalone/7.png"></figure></li>
</ol>
<p>配置log4j.properties 文件 [可选配置]’</p>
<ol>
<li>改名<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br></pre></td></tr></table></figure>
<figure class="figure"><img src="/../image-sparkstandalone/8.png"></figure></li>
<li>修改内容 参考下图</li>
</ol>
<p>将INFO修改为WARN</p>
<p><figure class="figure"><img src="/../image-sparkstandalone/9.png"></figure></p>
<p>将spark安装文件夹分发到其他服务器上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/</span><br><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/</span><br></pre></td></tr></table></figure>
<p>并在node2、node3给spark安装目录增加软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/10.png"></figure></p>
<p><figure class="figure"><img src="/../image-sparkstandalone/11.png"></figure><br>检查每台机器的<br>JAVA_HOME<br>SPARK_HOME<br>PYSPARK_PYTHON<br>等等 环境变量是否正常指向正确的目录<br>启动历史服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/12.png"></figure><br>启动spark的master和worker进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/13.png"></figure>	</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/14.png"></figure></p>
<p>查看master的WEB UI</p>
<p><figure class="figure"><img src="/../image-sparkstandalone/15.png"></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2023/06/13/Spark(local)/">Spark(local)</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2023-06-13</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/spark/" rel="tag">spark</a> <a class="article__tag-none-link" href="/tags/%E6%90%AD%E5%BB%BA/" rel="tag">搭建</a>
			</span>
		
	</div>

	

	
		<h1 id="Spark-local-模式"><a href="#Spark-local-模式" class="headerlink" title="Spark(local)模式"></a>Spark(local)模式</h1><h2 id="一、创建虚拟环境"><a href="#一、创建虚拟环境" class="headerlink" title="一、创建虚拟环境"></a>一、创建虚拟环境</h2><p>1、需将Python环境需要安装到Linux(虚拟机)和Windows(本机)上<br>在windows环境安装Anaconda，在三台服务器分别配置Anaconda<br>在Windows环境下，用Anaconda自带的命令行界面创建虚拟环境<br><figure class="figure"><img src="/../image-spark(local)/1.png"></figure></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8</span><br></pre></td></tr></table></figure>
<p>#切换到虚拟环境内</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark</span><br></pre></td></tr></table></figure>
<p>#在虚拟环境内安装包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple </span><br></pre></td></tr></table></figure>
<p>#切换pyspark版本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall pyspark</span><br><span class="line">pip install pyspark==3.2.0 -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<p>2、最终配置好的状态：<br><figure class="figure"><img src="/../image-spark(local)/2.png"></figure><br>3、最终配置好的状态：<br><figure class="figure"><img src="/../image-spark(local)/3.png"></figure><br>4、上传spark-3.2.0-bin-hadoop3.2.tgz到Linux服务器中<br>解压并创建软链接<br><figure class="figure"><img src="/../image-spark(local)/4.png"></figure><br>5、进行测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 bin]# ./pyspark</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-spark(local)/5.png"></figure></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-spark(local)/6.png"></figure><br>Web UI (4040)<br>Spark程序在运行的时候, 会绑定到机器的4040端口上.<br><figure class="figure"><img src="/../image-spark(local)/7.png"></figure><br>4040端口是一个WEBUI端口, 可以在浏览器内打开:<br><figure class="figure"><img src="/../image-spark(local)/8.png"></figure><br>打开监控页面后, 可以发现 在程序内仅有一个Driver<br>因为我们是Local模式, Driver即管理又干活<br>同时, 输入jps<br><figure class="figure"><img src="/../image-spark(local)/9.png"></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2023/06/07/hello-world/">Hello World</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2023-06-07</span><br />
		
		
	</div>

	

	
		<p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

	

	

</article>






	<span class="different-posts">📕 end of posts 📕</span>


	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2023 John Doe | Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> | Theme <a target="_blank" rel="noopener" href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
