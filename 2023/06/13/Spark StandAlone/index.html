<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Spark StandAlone搭建 | Hexo</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
<meta name="generator" content="Hexo 6.3.0"></head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/archives" class="header__link">Archive</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Hexo</a></h1>
		<h2 class="header__subtitle"></h2>
	</header>

	<main>
		<article>
	
		<h1>Spark StandAlone搭建</h1>
	
	<div class="article__infos">
		<span class="article__date">2023-06-13</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark/" rel="tag">Spark</a> <a class="article__tag-none-link" href="/tags/%E6%90%AD%E5%BB%BA/" rel="tag">搭建</a>
			</span>
		
	</div>

	

	
		<h1 id="Spark-StandAlone搭建"><a href="#Spark-StandAlone搭建" class="headerlink" title="Spark StandAlone搭建"></a>Spark StandAlone搭建</h1><ol>
<li>在所有机器上安装python（Anaconda）<br>同时不要忘记 都创建pysapek虚拟环境 以及安装虚拟环境所需要的包pyspark jieba pyhive<br>2 .在所有机器配置环境变量<br>参考local模式下环境变量的配置内容<br>3.配置配置文件<br>进入到spark的配置文件目录中, cd $SPARK_HOME&#x2F;conf<br>配置workers文件</li>
</ol>
<p>改名, 去掉后面的.template后缀</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/1.png"></figure></p>
<p>编辑worker文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim workers</span><br></pre></td></tr></table></figure>
<p>将里面的localhost删除, 追加</p>
<p>node1</p>
<p>node2</p>
<p>node3</p>
<p>到workers文件内</p>
<p><figure class="figure"><img src="/../image-sparkstandalone/2.png"></figure><br>配置spark-env.sh文件</p>
<ol>
<li><p>改名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/3.png"></figure></p>
</li>
<li><p>编辑spark-env.sh, 在底部追加如下内容</p>
</li>
</ol>
<p>设置JAVA安装目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/export/server/jdk</span><br></pre></td></tr></table></figure>
<p>HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>
<p>指定spark老大Master的IP和提交任务的通信端口<br>告知Spark的master运行在哪个机器上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_HOST=node1</span><br></pre></td></tr></table></figure>
<p>告知sparkmaster的通讯端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>
<p>告知spark master的 webui端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br></pre></td></tr></table></figure>
<p>worker cpu可用核数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_CORES=1</span><br></pre></td></tr></table></figure>
<p>worker可用内存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_MEMORY=1g</span><br></pre></td></tr></table></figure>
<p>worker的工作通讯地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_PORT=7078</span><br></pre></td></tr></table></figure>
<p>worker的 webui地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br></pre></td></tr></table></figure>
<h2 id="设置历史服务器"><a href="#设置历史服务器" class="headerlink" title="设置历史服务器"></a>设置历史服务器</h2><p>配置的意思是  将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/../image-sparkstandalone/4.png"></figure><br>在HDFS上创建程序运行历史记录存放的文件夹:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/5.png"></figure><br>配置spark-defaults.conf文件</p>
<ol>
<li>改名<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure>
<figure class="figure"><img src="/../image-sparkstandalone/6.png"></figure></li>
<li>修改内容, 追加如下内容<br>开启spark的日期记录功能<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled 	true</span><br></pre></td></tr></table></figure>
设置spark日志记录的路径<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.dir	 hdfs://node1:8020/sparklog/ </span><br></pre></td></tr></table></figure>
设置spark日志是否启动压缩<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.compress 	true</span><br></pre></td></tr></table></figure>
<figure class="figure"><img src="/../image-sparkstandalone/7.png"></figure></li>
</ol>
<p>配置log4j.properties 文件 [可选配置]’</p>
<ol>
<li>改名<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br></pre></td></tr></table></figure>
<figure class="figure"><img src="/../image-sparkstandalone/8.png"></figure></li>
<li>修改内容 参考下图</li>
</ol>
<p>将INFO修改为WARN</p>
<p><figure class="figure"><img src="/../image-sparkstandalone/9.png"></figure></p>
<p>将spark安装文件夹分发到其他服务器上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/</span><br><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/</span><br></pre></td></tr></table></figure>
<p>并在node2、node3给spark安装目录增加软连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/10.png"></figure></p>
<p><figure class="figure"><img src="/../image-sparkstandalone/11.png"></figure><br>检查每台机器的<br>JAVA_HOME<br>SPARK_HOME<br>PYSPARK_PYTHON<br>等等 环境变量是否正常指向正确的目录<br>启动历史服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/12.png"></figure><br>启动spark的master和worker进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/13.png"></figure>	</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>
<p><figure class="figure"><img src="/../image-sparkstandalone/14.png"></figure></p>
<p>查看master的WEB UI</p>
<p><figure class="figure"><img src="/../image-sparkstandalone/15.png"></figure></p>

	

	
		<span class="different-posts"><a href="/2023/06/13/Spark%20StandAlone/" onclick="window.history.go(-1); return false;">⬅️ Go back </a></span>

	

</article>

	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2023 John Doe | Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> | Theme <a target="_blank" rel="noopener" href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
